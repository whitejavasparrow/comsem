# -*- coding: utf-8 -*-
"""ComSem-1005.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15AFyZuyiMgzHvc7uh5NvopVnmS4UWPI5

# Computational Semantics - 191005
## Enviornment Setup
"""

#!pip install requests nltk spacy beautifulsoup4
#!python3 -m spacy download en_core_web_sm

"""## Importing packages"""

from collections import Counter
import nltk
import spacy
import requests
from nltk.corpus import wordnet as wn
from bs4 import BeautifulSoup
nltk.download("wordnet")

"""# Data preparation

## Loading lyric data
"""

LYRICS_URI = "https://www.lyrics.com/lyric/5645831/Terry+Hall/All+Kinds+of+Everything"
lyrics_resp = requests.get(LYRICS_URI)

lyrics_resp.status_code

"""## Parsing HTML"""

lyrics_page = BeautifulSoup(lyrics_resp.text)

lyrics = lyrics_page.select_one("#lyric-body-text").get_text()

print(lyrics)

"""# A WordNet approach to computational semantics

## NLP preprocessing
"""

nlp = spacy.load("en_core_web_sm")
doc = nlp(lyrics)

"""## Find hypernyms paths of nouns"""

syn_counter = Counter()
hypernym_paths = []
for tok in doc:
    if tok.pos_ != "NOUN": continue    
    lemma_x = tok.lemma_
    syn_x = wn.synsets(lemma_x)
    if not syn_x: continue
    paths = syn_x[0].hypernym_paths()
    if not paths: continue
    hypernyms = paths[0][-3:-1]
    hypernym_paths.append((lemma_x, *hypernyms))
    syn_counter.update(hypernyms)

"""## Hypernym counts of nouns"""

hypernym_paths

syn_counter.most_common(20)